# ğŸ­ Playwright Recursive Crawler

Playwright tabanlÄ±, headless tarayÄ±cÄ± kullanarak web sitelerini recursive olarak tarayan ve tÃ¼m network dosyalarÄ±nÄ± kaydeden gÃ¼Ã§lÃ¼ bir crawler.

## âœ¨ Ã–zellikler

- **Playwright** ile gerÃ§ek tarayÄ±cÄ± simÃ¼lasyonu
- **Network yakalama** - TÃ¼m CSS, JS, resim, font dosyalarÄ±nÄ± kaydet
- **Recursive crawling** - Derinlik kontrolÃ¼ ile linkler arasÄ±nda gezinme
- **Queue sistemi** - Duplicate URL kontrolÃ¼
- **Filtering** - Include/exclude pattern desteÄŸi
- **Same-domain** - Sadece aynÄ± domain veya tÃ¼m domainler
- **TypeScript** - Tam tip gÃ¼venliÄŸi

## ğŸ“¦ Kurulum

```bash
# BaÄŸÄ±mlÄ±lÄ±klarÄ± yÃ¼kle
npm install

# Playwright tarayÄ±cÄ±larÄ±nÄ± yÃ¼kle
npm run install:browsers

# TypeScript build
npm run build
```

## ğŸš€ KullanÄ±m

### CLI ile

```bash
# Basit kullanÄ±m
npm run crawl -- -u https://example.com

# DetaylÄ± kullanÄ±m
npm run crawl -- \
  -u https://example.com \
  -o ./my-output \
  -d 5 \
  -p 200 \
  --delay 2000 \
  --same-domain

# TÃ¼m domainleri dahil et
npm run crawl -- -u https://example.com --all-domains

# Pattern filtering
npm run crawl -- -u https://example.com \
  --exclude ".*\\.pdf$" ".*login.*" \
  --include ".*\\.html$" ".*\\.php$"
```

### Programatik KullanÄ±m

```typescript
import { PlaywrightCrawler } from './crawler.js';

const crawler = new PlaywrightCrawler({
  startUrl: 'https://example.com',
  outputDir: './crawled',
  maxDepth: 3,
  maxPages: 100,
  sameDomainOnly: true,
  delay: 1000,
  excludePatterns: [/\.pdf$/, /login/],
  includePatterns: [/\.html$/]
});

await crawler.initialize();
await crawler.start();
await crawler.close();
```

## ğŸ“ Ã‡Ä±ktÄ± YapÄ±sÄ±

```
crawled/
â”œâ”€â”€ html/           # HTML dosyalarÄ±
â”‚   â”œâ”€â”€ index.html
â”‚   â””â”€â”€ about.html
â”œâ”€â”€ css/            # CSS dosyalarÄ±
â”œâ”€â”€ js/             # JavaScript dosyalarÄ±
â”œâ”€â”€ images/         # Resim dosyalarÄ±
â”œâ”€â”€ fonts/          # Font dosyalarÄ±
â”œâ”€â”€ media/          # Video/audio dosyalarÄ±
â””â”€â”€ other/          # DiÄŸer dosyalar
```

## âš™ï¸ CLI SeÃ§enekleri

| Parametre | KÄ±saltma | AÃ§Ä±klama | VarsayÄ±lan |
|-----------|----------|----------|------------|
| `--url` | `-u` | BaÅŸlangÄ±Ã§ URL (zorunlu) | - |
| `--output` | `-o` | Ã‡Ä±ktÄ± dizini | `./crawled` |
| `--depth` | `-d` | Maksimum derinlik | `3` |
| `--pages` | `-p` | Maksimum sayfa sayÄ±sÄ± | `100` |
| `--same-domain` | - | Sadece aynÄ± domain | `true` |
| `--all-domains` | - | TÃ¼m domainleri dahil et | `false` |
| `--delay` | - | Ä°stekler arasÄ± gecikme (ms) | `1000` |
| `--timeout` | - | Sayfa timeout (ms) | `30000` |
| `--exclude` | - | HariÃ§ tutulacak pattern'ler | `[]` |
| `--include` | - | Dahil edilecek pattern'ler | `[]` |

## ğŸ¯ KullanÄ±m SenaryolarÄ±

### 1. Statik Site ArÅŸivleme
```bash
npm run crawl -- -u https://mysite.com -d 10 -p 1000
```

### 2. Tek Sayfa Crawl (Depth=0)
```bash
npm run crawl -- -u https://example.com -d 0
```

### 3. Sadece HTML ve CSS
```bash
npm run crawl -- -u https://example.com \
  --include ".*\\.(html|css)$"
```

### 4. Login/Admin SayfalarÄ±nÄ± HariÃ§ Tut
```bash
npm run crawl -- -u https://example.com \
  --exclude ".*login.*" ".*admin.*"
```

## ğŸ”§ Mimari

```
src/
â”œâ”€â”€ index.ts              # CLI entry point
â”œâ”€â”€ crawler.ts            # Ana crawler sÄ±nÄ±fÄ±
â”œâ”€â”€ types.ts              # TypeScript tip tanÄ±mlarÄ±
â””â”€â”€ utils/
    â”œâ”€â”€ file-saver.ts     # Dosya kaydetme logic
    â”œâ”€â”€ url-queue.ts      # URL queue yÃ¶netimi
    â””â”€â”€ link-parser.ts    # Link bulma ve filtering
```

## ğŸ“Š Network Yakalama

Crawler tÃ¼m network isteklerini yakalar:

- âœ… **Images**: jpg, png, gif, svg, webp, ico
- âœ… **Styles**: css
- âœ… **Scripts**: js, mjs, jsx
- âœ… **Fonts**: woff, woff2, ttf, otf
- âœ… **Media**: mp4, webm, mp3, wav
- âœ… **HTML**: Sayfa iÃ§erikleri

## ğŸ›¡ï¸ Best Practices

1. **Rate Limiting**: `--delay` parametresi ile sunucuya yÃ¼k bindirmeyin
2. **Timeout**: YavaÅŸ sitelerde `--timeout` deÄŸerini artÄ±rÄ±n
3. **Max Pages**: BÃ¼yÃ¼k sitelerde `--pages` ile limit koyun
4. **Filtering**: Gereksiz dosyalarÄ± `--exclude` ile filtreleyin
5. **robots.txt**: Sitenin robots.txt kurallarÄ±na uyun

## ğŸ› Troubleshooting

### TarayÄ±cÄ± HatasÄ±
```bash
# Playwright tarayÄ±cÄ±larÄ±nÄ± yeniden yÃ¼kle
npm run install:browsers
```

### Memory HatasÄ±
```bash
# Max pages deÄŸerini dÃ¼ÅŸÃ¼r
npm run crawl -- -u https://example.com -p 50
```

### Network Timeout
```bash
# Timeout deÄŸerini artÄ±r
npm run crawl -- -u https://example.com --timeout 60000
```

## ğŸ“ Lisans

MIT

## ğŸ¤ KatkÄ±

Pull request'ler memnuniyetle karÅŸÄ±lanÄ±r!
